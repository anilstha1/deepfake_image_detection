{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "NZIGALfevRS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"xhlulu/140k-real-and-fake-faces\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "id": "y9U5HYZgvrpp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "print(torch.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "print(\"GPU:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"None\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "wOK4dQFAv7fT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import first dataset\n",
        "import pandas as pd\n",
        "\n",
        "df_train = pd.read_csv(f'{path}/train.csv',index_col=0)\n",
        "df_val = pd.read_csv(f'{path}/valid.csv',index_col=0)\n",
        "df_test = pd.read_csv(f'{path}/test.csv',index_col=0)"
      ],
      "metadata": {
        "id": "YycIE550wQFY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.head()"
      ],
      "metadata": {
        "id": "G5qW4UYxwd8t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_val.head()"
      ],
      "metadata": {
        "id": "a8EZEImpwhkq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test.head()"
      ],
      "metadata": {
        "id": "AXabeAddwmNr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remove not needed columns and shuffle\n",
        "df_train = df_train.drop(columns=['original_path','id','label_str']).sample(frac=1)\n",
        "df_val = df_val.drop(columns=['original_path','id','label_str']).sample(frac=1)\n",
        "df_test = df_test.drop(columns=['original_path','id','label_str']).sample(frac=1)\n",
        "df_train.head()"
      ],
      "metadata": {
        "id": "JAY7fID_wpph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# rectify labels\n",
        "df_train['label'] = 1 - df_train['label']\n",
        "df_val['label'] = 1 - df_val['label']\n",
        "df_test['label'] = 1 - df_test['label']\n",
        "df_train.head()"
      ],
      "metadata": {
        "id": "UXP7I6sjwy4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# map each image name to its path\n",
        "df_train[\"path\"] = df_train[\"path\"].map(lambda x: f'{path}/real_vs_fake/real-vs-fake/' + x)\n",
        "df_val[\"path\"]= df_val[\"path\"].map(lambda x: f'{path}/real_vs_fake/real-vs-fake/' + x)\n",
        "df_test[\"path\"] = df_test[\"path\"].map(lambda x: f'{path}/real_vs_fake/real-vs-fake/' + x)\n",
        "df_train.head()"
      ],
      "metadata": {
        "id": "egLyvLl8w9NI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# show a sample of images from the dataset with corresponding labels\n",
        "import matplotlib.pyplot as plt\n",
        "label_arg = {0:'real', 1:'fake'}\n",
        "fig, axs = plt.subplots(2,3,figsize=(10,8))\n",
        "for i in range(2):\n",
        "    for j in range(3):\n",
        "        random_idx = np.random.randint(0,df_train.shape[0],1)\n",
        "        img = Image.open(df_train[\"path\"].iloc[int(random_idx)])\n",
        "        label = df_train['label'].iloc[int(random_idx)]\n",
        "        axs[i,j].imshow(img)\n",
        "        axs[i,j].set_title(label_arg[label])"
      ],
      "metadata": {
        "id": "3hJKCwuZxFv4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define transforms with input size 224x224 and standard normalization\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),  # ImageNet normalization\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n"
      ],
      "metadata": {
        "id": "lkzqEp5nxTqx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset class\n",
        "class DeepfakeDataset(Dataset):\n",
        "    def __init__(self, df, transform=None):\n",
        "        self.df = df\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        img_path = row[\"path\"]\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        label = row[\"label\"]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "# Create datasets and dataloaders (assuming df_train, df_val, df_test are already defined)\n",
        "train_dataset = DeepfakeDataset(df_train, transform=train_transform)\n",
        "val_dataset   = DeepfakeDataset(df_val, transform=test_transform)\n",
        "test_dataset  = DeepfakeDataset(df_test, transform=test_transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
        "val_loader   = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
        "test_loader  = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
        "\n",
        "# Check shapes (for sanity)\n",
        "for images, labels in train_loader:\n",
        "    print(images.shape)  # Expect [batch_size, 3, 224, 224]\n",
        "    print(labels.shape)  # Expect [batch_size]\n",
        "    break\n"
      ],
      "metadata": {
        "id": "7pJM0YncxaHt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the drop_connect function for stochastic depth (as used in EfficientNet)\n",
        "def drop_connect(inputs: torch.Tensor, p: float, training: bool) -> torch.Tensor:\n",
        "    if not training or p <= 0.0:\n",
        "        return inputs\n",
        "    keep_prob = 1.0 - p\n",
        "    batch_size = inputs.shape[0]\n",
        "    # Generate binary mask\n",
        "    random_tensor = keep_prob + torch.rand([batch_size, 1, 1, 1], dtype=inputs.dtype, device=inputs.device)\n",
        "    binary_tensor = torch.floor(random_tensor)\n",
        "    # Scale output\n",
        "    outputs = inputs.div(keep_prob) * binary_tensor\n",
        "    return outputs\n",
        "\n",
        "# Squeeze-and-Excitation block\n",
        "class SqueezeExcitation(nn.Module):\n",
        "    def __init__(self, in_ch: int, se_ratio: float = 0.25, activation=nn.SiLU):\n",
        "        super().__init__()\n",
        "        squeezed_ch = max(1, int(in_ch * se_ratio))\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, squeezed_ch, kernel_size=1),\n",
        "            activation(),\n",
        "            nn.Conv2d(squeezed_ch, in_ch, kernel_size=1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        s = self.avg_pool(x)\n",
        "        s = self.fc(s)\n",
        "        return x * s\n",
        "\n",
        "# MBConvBlock (Mobile Inverted Residual Block with SE and drop connect)\n",
        "class MBConvBlock(nn.Module):\n",
        "    def __init__(self, in_ch: int, out_ch: int, kernel_size: int, stride: int,\n",
        "                 expand_ratio: int, se_ratio: float = 0.25, drop_connect_rate: float = 0.0,\n",
        "                 norm_layer=nn.BatchNorm2d, activation=nn.SiLU):\n",
        "        super().__init__()\n",
        "        self.in_ch = in_ch\n",
        "        self.out_ch = out_ch\n",
        "        self.stride = stride\n",
        "        self.expand_ratio = expand_ratio\n",
        "        self.has_residual = (stride == 1 and in_ch == out_ch)\n",
        "        self.drop_connect_rate = drop_connect_rate\n",
        "\n",
        "        hidden_dim = in_ch * expand_ratio\n",
        "        layers = []\n",
        "\n",
        "        # Expansion phase\n",
        "        if expand_ratio != 1:\n",
        "            layers += [\n",
        "                nn.Conv2d(in_ch, hidden_dim, kernel_size=1, bias=False),\n",
        "                norm_layer(hidden_dim),\n",
        "                activation(),\n",
        "            ]\n",
        "\n",
        "        # Depthwise convolution\n",
        "        layers += [\n",
        "            nn.Conv2d(hidden_dim, hidden_dim, kernel_size=kernel_size, stride=stride,\n",
        "                      padding=kernel_size // 2, groups=hidden_dim, bias=False),\n",
        "            norm_layer(hidden_dim),\n",
        "            activation(),\n",
        "        ]\n",
        "\n",
        "        # Squeeze-and-Excitation\n",
        "        if se_ratio is not None and 0.0 < se_ratio <= 1.0:\n",
        "            layers.append(SqueezeExcitation(hidden_dim, se_ratio=se_ratio, activation=activation))\n",
        "\n",
        "        # Projection phase\n",
        "        layers += [\n",
        "            nn.Conv2d(hidden_dim, out_ch, kernel_size=1, bias=False),\n",
        "            norm_layer(out_ch),\n",
        "        ]\n",
        "\n",
        "        self.block = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        out = self.block(x)\n",
        "        if self.has_residual:\n",
        "            if self.drop_connect_rate and self.training:\n",
        "                out = drop_connect(out, p=self.drop_connect_rate, training=self.training)\n",
        "            out = out + x\n",
        "        return out\n",
        "\n",
        "# EfficientNet-B0 definition\n",
        "class EfficientNetB0(nn.Module):\n",
        "    \"\"\"\n",
        "    EfficientNet-B0 model without pretrained weights. Uses MBConv blocks and SE blocks.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes: int = 1, drop_connect_rate: float = 0.2):\n",
        "        super().__init__()\n",
        "        norm_layer = nn.BatchNorm2d\n",
        "        activation = nn.SiLU\n",
        "\n",
        "        # Stem: initial convolution layer\n",
        "        self.stem = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1, bias=False),\n",
        "            norm_layer(32),\n",
        "            activation(),\n",
        "        )\n",
        "\n",
        "        # EfficientNet-B0 block settings: (expansion, out_channels, repeats, kernel, stride)\n",
        "        b0_settings = [\n",
        "            (1, 16, 1, 3, 1),\n",
        "            (6, 24, 2, 3, 2),\n",
        "            (6, 40, 2, 5, 2),\n",
        "            (6, 80, 3, 3, 2),\n",
        "            (6, 112, 3, 5, 1),\n",
        "            (6, 192, 4, 5, 2),\n",
        "            (6, 320, 1, 3, 1),\n",
        "        ]\n",
        "\n",
        "        # Build MBConv blocks\n",
        "        blocks = []\n",
        "        in_ch = 32\n",
        "        total_blocks = sum([r for (_, _, r, _, _) in b0_settings])\n",
        "        block_id = 0\n",
        "        for expansion, out_ch, repeats, k, s in b0_settings:\n",
        "            for i in range(repeats):\n",
        "                stride = s if i == 0 else 1\n",
        "                # Linearly scale drop connect rate\n",
        "                dcr = drop_connect_rate * float(block_id) / max(1, total_blocks - 1)\n",
        "                blocks.append(\n",
        "                    MBConvBlock(in_ch=in_ch, out_ch=out_ch,\n",
        "                                kernel_size=k, stride=stride,\n",
        "                                expand_ratio=expansion, se_ratio=0.25,\n",
        "                                drop_connect_rate=dcr,\n",
        "                                norm_layer=norm_layer, activation=activation)\n",
        "                )\n",
        "                in_ch = out_ch\n",
        "                block_id += 1\n",
        "\n",
        "        self.blocks = nn.Sequential(*blocks)\n",
        "\n",
        "        # Head: final layers\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, 1280, kernel_size=1, bias=False),\n",
        "            norm_layer(1280),\n",
        "            activation(),\n",
        "        )\n",
        "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(p=0.2),\n",
        "            nn.Linear(1280, num_classes)\n",
        "        )\n",
        "\n",
        "        # Initialize weights\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.stem(x)\n",
        "        x = self.blocks(x)\n",
        "        x = self.head(x)\n",
        "        x = self.pool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.ones_(m.weight)\n",
        "                nn.init.zeros_(m.bias)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, 0, 0.01)\n",
        "                nn.init.zeros_(m.bias)\n"
      ],
      "metadata": {
        "id": "jyawYJT4xf6I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the custom EfficientNet-B0 for binary classification (num_classes=1)\n",
        "channels = 3\n",
        "num_classes = 1\n",
        "model = EfficientNetB0(num_classes=num_classes).to(device)\n",
        "print(model)\n"
      ],
      "metadata": {
        "id": "IwctRSP3xmCZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use BCEWithLogitsLoss for binary classification and compute pos_weight if needed\n",
        "learning_rate=1e-3\n",
        "num_pos = df_train['label'].sum()\n",
        "num_neg = len(df_train) - num_pos\n",
        "pos_weight = torch.tensor([num_neg / num_pos]).to(device)\n",
        "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n"
      ],
      "metadata": {
        "id": "52wkJ1rvxrMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "num_epochs = 20\n",
        "start_epoch = 0\n",
        "checkpoint_path = \"checkpoint.pth\"\n",
        "\n",
        "for epoch in range(start_epoch, num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    train_pbar = tqdm(train_loader, desc=f\"Epoch [{epoch+1}/{num_epochs}] Training\", leave=False)\n",
        "    for images, labels in train_pbar:\n",
        "        images = images.to(device)\n",
        "        labels = labels.float().unsqueeze(1).to(device)  # BCEWithLogitsLoss expects float\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)                   # raw logits\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        # Apply sigmoid to get probabilities, then threshold\n",
        "        preds = (torch.sigmoid(outputs) > 0.5).float()\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "        train_pbar.set_postfix({\n",
        "            \"Loss\": f\"{loss.item():.4f}\",\n",
        "            \"Acc\": f\"{(correct / total):.4f}\"\n",
        "        })\n",
        "\n",
        "    train_loss = running_loss / total\n",
        "    train_acc = correct / total\n",
        "\n",
        "    # -------------------- Validation --------------------\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    val_correct = 0\n",
        "    val_total = 0\n",
        "\n",
        "    all_labels = []\n",
        "    all_probs = []\n",
        "\n",
        "    val_pbar = tqdm(val_loader, desc=f\"Epoch [{epoch+1}/{num_epochs}] Validation\", leave=False)\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_pbar:\n",
        "            images = images.to(device)\n",
        "            labels = labels.float().unsqueeze(1).to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            val_loss += loss.item() * images.size(0)\n",
        "            preds = (torch.sigmoid(outputs) > 0.5).float()\n",
        "            val_correct += (preds == labels).sum().item()\n",
        "            val_total += labels.size(0)\n",
        "\n",
        "            val_pbar.set_postfix({\n",
        "                \"Val Loss\": f\"{loss.item():.4f}\",\n",
        "                \"Val Acc\": f\"{(val_correct / val_total):.4f}\"\n",
        "            })\n",
        "\n",
        "            # Collect labels and predicted probabilities\n",
        "            all_probs.extend(torch.sigmoid(outputs).cpu().numpy().flatten())\n",
        "            all_labels.extend(labels.cpu().numpy().flatten())\n",
        "\n",
        "    val_loss /= val_total\n",
        "    val_acc = val_correct / val_total\n",
        "\n",
        "    print(f\"\\nEpoch [{epoch+1}/{num_epochs}] \"\n",
        "          f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} \"\n",
        "          f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "    # Compute ROC AUC if needed\n",
        "    all_labels_np = np.array(all_labels)\n",
        "    all_probs_np = np.array(all_probs)\n",
        "    fpr, tpr, _ = roc_curve(all_labels_np, all_probs_np)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    print(f\"Epoch {epoch+1} AUC: {roc_auc:.4f}\")\n",
        "\n",
        "    # (Optional) Save checkpoint\n",
        "    torch.save({'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict()},\n",
        "               checkpoint_path)\n",
        "    print(f\"Checkpoint saved at epoch {epoch+1}\")\n"
      ],
      "metadata": {
        "id": "ix10uCNRyCSK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "test_loss = 0.0\n",
        "test_correct = 0\n",
        "test_total = 0\n",
        "\n",
        "all_test_labels = []\n",
        "all_test_probs = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.float().unsqueeze(1).to(device)\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        test_loss += loss.item() * images.size(0)\n",
        "        preds = (torch.sigmoid(outputs) > 0.5).float()\n",
        "        test_correct += (preds == labels).sum().item()\n",
        "        test_total += labels.size(0)\n",
        "\n",
        "        all_test_probs.extend(torch.sigmoid(outputs).cpu().numpy().flatten())\n",
        "        all_test_labels.extend(labels.cpu().numpy().flatten())\n",
        "\n",
        "test_loss /= test_total\n",
        "test_acc = test_correct / test_total\n",
        "print(f\"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}\")\n",
        "\n",
        "# Optional: compute test ROC AUC\n",
        "fpr, tpr, _ = roc_curve(np.array(all_test_labels), np.array(all_test_probs))\n",
        "test_auc = auc(fpr, tpr)\n",
        "print(f\"Test AUC: {test_auc:.4f}\")\n"
      ],
      "metadata": {
        "id": "MqWezrHp2oSt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}